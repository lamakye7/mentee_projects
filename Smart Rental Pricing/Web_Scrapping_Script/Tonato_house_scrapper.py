# -*- coding: utf-8 -*-
"""house_scrapper.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rYhmrKlb5pS_dmeiDLeiGleDWCu4Ndtz
"""

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException
import time
import pandas as pd
import re
import logging

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def setup_driver():
    """Setup and return a Chrome WebDriver with appropriate options for production."""
    chrome_options = Options()
    chrome_options.add_argument("--headless")  # Run in headless mode for production
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--window-size=1920,1080")
    chrome_options.add_argument("--incognito") # Use incognito mode to avoid caching
    chrome_options.add_argument("--log-level=3") # Suppress verbose logs from Chrome
    # Add a user-agent to mimic a real browser
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36")

    try:
        driver = webdriver.Chrome(options=chrome_options)
        return driver
    except WebDriverException as e:
        logging.error(f"Failed to set up WebDriver: {e}")
        raise

def get_last_page_number(driver_instance, base_url):
    """
    Attempts to find the last page number from the pagination controls.
    """
    logging.info("Attempting to determine the last page number...")
    driver_instance.get(base_url)
    try:
        # Wait for pagination controls to be visible
        WebDriverWait(driver_instance, 15).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "div.pagination__button"))
        )

        # Look for the last page number directly
        last_page_link = driver_instance.find_elements(By.CSS_SELECTOR, "div.pagination__button a")
        if last_page_link:
            # Filter out "Previous" and "Next" links and find the maximum page number
            page_numbers = []
            for link in last_page_link:
                try:
                    # Extract page number from href or text
                    href = link.get_attribute('href')
                    if href:
                        match = re.search(r'page=(\d+)', href)
                        if match:
                            page_numbers.append(int(match.group(1)))
                    if link.text.isdigit():
                        page_numbers.append(int(link.text))
                except ValueError:
                    continue # Ignore non-numeric text

            if page_numbers:
                max_page = max(page_numbers)
                logging.info(f"Dynamically determined last page as: {max_page}")
                return max_page

        # Fallback if direct last page link is not found, but pagination exists
        logging.warning("Could not find direct last page link. Checking for '...' and 'Next' button.")

        # Check for ellipsis followed by a number
        ellipsis_sibling = driver_instance.find_elements(By.XPATH, "//span[text()='...']/following-sibling::a[last()]")
        if ellipsis_sibling:
             try:
                 page_num = int(ellipsis_sibling[0].text)
                 logging.info(f"Determined last page from ellipsis sibling: {page_num}")
                 return page_num
             except ValueError:
                 pass # Not a number

        # If all else fails, and we can't find a definitive last page,
        # we might assume a reasonable default or raise an error.
        logging.warning("Could not definitively determine the last page number. Defaulting to a fixed number (e.g., 500) if not explicitly set, or consider if this is the only page.")
        # Given the previous context, 499 was the last page, so let's use a safe upper bound
        return 500

    except TimeoutException:
        logging.warning("Pagination elements not found within timeout. Assuming only one page or a different pagination structure.")
        return 1 # Assume only one page if pagination isn't present
    except Exception as e:
        logging.error(f"An error occurred while trying to find the last page: {e}")
        return 1 # Default to 1 page on error

def scrape_tonaton_houses(base_url, max_pages=None):
    """
    Scrape house listings from Tonaton website, dynamically finding the last page.

    Args:
        base_url: The URL of the Tonaton house listings page (e.g., "https://tonaton.com/c_houses-apartments-for-rent")
        max_pages: Optional. If provided, scrapes up to this number of pages.
                   If None, it will try to determine the last page dynamically.

    Returns:
        DataFrame containing the scraped house data
    """
    all_houses = []
    driver_main_page = None # Initialize to None

    try:
        # Determine the total number of pages to scrape
        if max_pages is None:
            driver_main_page = setup_driver()
            total_pages_to_scrape = get_last_page_number(driver_main_page, base_url)
            driver_main_page.quit() # Close driver used for page count
            driver_main_page = None # Reset
        else:
            total_pages_to_scrape = max_pages

        logging.info(f"Will attempt to scrape a total of {total_pages_to_scrape} pages.")

        for page in range(1, total_pages_to_scrape + 1):
            page_url = f"{base_url}?page={page}" if page > 1 else base_url
            logging.info(f"\n==== Scraping page {page}/{total_pages_to_scrape}: {page_url} ====")

            driver_main_page = setup_driver() # Fresh driver for each main page
            try:
                driver_main_page.get(page_url)

                # Wait for listings to load
                try:
                    WebDriverWait(driver_main_page, 20).until(
                        EC.presence_of_element_located((By.CSS_SELECTOR, "div.product__container"))
                    )
                except TimeoutException:
                    logging.warning(f"Timeout waiting for listings to load on page {page}. Moving to next page.")
                    continue

                time.sleep(3) # Allow extra time for dynamic content

                product_containers = driver_main_page.find_elements(By.CSS_SELECTOR, "div.product__container")
                logging.info(f"Found {len(product_containers)} potential listings on page {page}")

                if not product_containers:
                    logging.warning(f"No listings found on page {page}. It might be the last page or an error occurred. Skipping.")
                    # Optionally, save page source for debugging
                    # with open(f"page_{page}_source.html", "w", encoding="utf-8") as f:
                    #     f.write(driver_main_page.page_source)
                    # logging.info(f"Saved page source to page_{page}_source.html for debugging.")
                    continue

                listing_urls = []
                for container in product_containers:
                    try:
                        link_element = container.find_element(By.TAG_NAME, "a")
                        listing_url = link_element.get_attribute('href')
                        if listing_url and listing_url not in listing_urls:
                            listing_urls.append(listing_url)
                    except NoSuchElementException:
                        logging.warning("Link element not found in a product container.")
                    except Exception as e:
                        logging.error(f"Error getting listing URL from container: {e}")

                logging.info(f"Identified {len(listing_urls)} unique listing URLs on page {page}.")

                # Process each listing URL with a fresh driver
                for idx, listing_url in enumerate(listing_urls):
                    house_data = {'url': listing_url, 'page': page}
                    logging.info(f"Processing listing {idx+1}/{len(listing_urls)} on page {page}: {listing_url}")

                    driver_detail = setup_driver() # Fresh driver for each detail page
                    try:
                        driver_detail.get(listing_url)

                        # Wait for the detailed page to load specific elements
                        try:
                            WebDriverWait(driver_detail, 20).until(
                                EC.presence_of_element_located((By.CSS_SELECTOR, "div.location"))
                            )
                        except TimeoutException:
                            logging.warning(f"Timeout loading detail page for {listing_url}. Skipping this listing.")
                            continue

                        time.sleep(2) # Extra time for JavaScript content on detail page

                        # Extract title and location
                        try:
                            location_div = driver_detail.find_element(By.CSS_SELECTOR, "div.location")
                            title_element = location_div.find_element(By.TAG_NAME, "h1")
                            location_element = location_div.find_element(By.CSS_SELECTOR, "div.flex.items-center.details__location span")
                            house_data['title'] = title_element.text.strip()
                            house_data['location'] = location_element.text.strip()
                            logging.debug(f"Title: {house_data['title']}, Location: {house_data['location']}")
                        except NoSuchElementException:
                            logging.warning(f"Title or location not found for {listing_url}.")
                            house_data['title'] = "N/A"
                            house_data['location'] = "N/A"

                        # Extract price (more robustly)
                        house_data['currency'] = "N/A"
                        house_data['price_value'] = "N/A"
                        house_data['price_formatted'] = "N/A"
                        try:
                            price_elements = driver_detail.find_elements(By.XPATH, "//*[contains(text(), 'GH₵')]")
                            for price_el in price_elements:
                                price_text = price_el.text.strip()
                                price_match = re.search(r'(GH₵\s*[\d,\.]+)', price_text) # Handles both comma and dot as decimal
                                if price_match:
                                    price_formatted = price_match.group(1)
                                    house_data['currency'] = "GH₵"
                                    # Remove currency symbol, commas, and convert to float (or keep as string if preferred)
                                    price_value_raw = price_formatted.replace('GH₵', '').strip().replace(',', '')
                                    # Handle cases where price might have decimal like 123.00
                                    if '.' in price_value_raw:
                                        house_data['price_value'] = float(price_value_raw)
                                    else:
                                        house_data['price_value'] = int(price_value_raw)
                                    house_data['price_formatted'] = price_formatted
                                    logging.debug(f"Price: {price_formatted}")
                                    break # Found price, exit loop
                            if house_data['price_formatted'] == "N/A":
                                logging.warning(f"Price not found after multiple attempts for {listing_url}")
                        except Exception as e:
                            logging.error(f"Error extracting price for {listing_url}: {e}")

                        # Extract detailed property attributes
                        try:
                            description_div = driver_detail.find_element(By.CSS_SELECTOR, "div.details__description.grid")
                            attribute_divs = description_div.find_elements(By.CSS_SELECTOR, "div")

                            for attr_div in attribute_divs:
                                try:
                                    title_p = attr_div.find_element(By.CSS_SELECTOR, "p.details__description__attribute-title")
                                    value_p = attr_div.find_element(By.CSS_SELECTOR, "p.details__description__attribute-value")

                                    attr_value = title_p.text.strip()
                                    attr_name = value_p.text.strip().lower().replace(' ', '_')

                                    house_data[attr_name] = attr_value

                                    # Map common attributes to standard field names (optional, can be done later too)
                                    if "bedrooms" in attr_name:
                                        house_data['bedrooms'] = attr_value
                                    elif "bathrooms" in attr_name:
                                        house_data['bathrooms'] = attr_value
                                    elif "toilets" in attr_name:
                                        house_data['toilets'] = attr_value
                                    elif "property_size" in attr_name:
                                        house_data['size'] = attr_value
                                    elif "property_type" in attr_name:
                                        house_data['property_type'] = attr_value
                                    elif "furnishing" in attr_name:
                                        house_data['furnished'] = attr_value
                                    elif "property_address" in attr_name:
                                        house_data['address'] = attr_value
                                    elif "facilities" in attr_name:
                                        house_data['facilities'] = attr_value
                                    elif "condition" in attr_name:
                                        house_data['condition'] = attr_value

                                except NoSuchElementException:
                                    continue # This div might not contain a title and value
                            logging.debug(f"Extracted {len(attribute_divs)} property attributes.")
                        except NoSuchElementException:
                            logging.warning(f"Property attributes section not found for {listing_url}.")
                        except Exception as e:
                            logging.error(f"Error getting property attributes for {listing_url}: {e}")

                        # Extract image URL
                        try:
                            img_element = driver_detail.find_element(By.CSS_SELECTOR, "img.carousel__img")
                            house_data['image_url'] = img_element.get_attribute('src')
                        except NoSuchElementException:
                            house_data['image_url'] = "N/A"
                            logging.warning(f"Image URL not found for {listing_url}.")

                        # Extract description
                        try:
                            description_element = driver_detail.find_element(By.CSS_SELECTOR, "div.description")
                            house_data['description'] = description_element.text.strip()
                        except NoSuchElementException:
                            house_data['description'] = "N/A"
                            logging.warning(f"Description not found for {listing_url}.")

                    except Exception as e:
                        logging.error(f"Error processing detailed page {listing_url}: {e}")
                    finally:
                        if driver_detail:
                            driver_detail.quit() # Always close the detail driver

                    all_houses.append(house_data)
                    time.sleep(1) # Delay between processing listings

                logging.info(f"Completed scraping {len(listing_urls)} listings from page {page}.")

            except Exception as e:
                logging.error(f"Error encountered on main page {page}: {e}")
            finally:
                if driver_main_page:
                    driver_main_page.quit() # Always close the main page driver

            time.sleep(3) # Delay between pages

    except Exception as e:
        logging.critical(f"A critical error occurred in the main scraping process: {e}", exc_info=True)
    finally:
        # Ensure any remaining drivers are closed
        if driver_main_page:
            driver_main_page.quit()

    df = pd.DataFrame(all_houses)
    return df

def main():
    base_url = "https://tonaton.com/c_houses-apartments-for-rent" # Base URL without page parameter

    logging.info("Starting to scrape Tonaton houses for rent...")

    try:
        # Set max_pages to None to enable dynamic last page detection.
        # You can set it to an integer (e.g., 50) if you want to limit the scrape.
        houses_df = scrape_tonaton_houses(base_url, max_pages=None)

        if houses_df.empty:
            logging.warning("No houses were scraped. Check selectors, URL, or website structure.")
            return

        # Define the output CSV file name
        output_csv_file = "tonaton_houses_for_rent_production.csv"
        houses_df.to_csv(output_csv_file, index=False, encoding='utf-8')

        logging.info(f"Scraping complete. Found {len(houses_df)} listings.")
        logging.info(f"Data saved to {output_csv_file}")

        if 'page' in houses_df.columns:
            page_counts = houses_df['page'].value_counts().sort_index()
            logging.info("\nListings distribution per page:")
            for page, count in page_counts.items():
                logging.info(f"Page {page}: {count} listings")

    except Exception as e:
        logging.critical(f"Error in main execution: {e}", exc_info=True)

if __name__ == "__main__":
    main()